---
title: "A Class-level Analysis about the Effects of Class Type on Average Math Scores"
author: ""
date: ""
output:
  pdf_document: default
  html_document:
    df_print: paged
    fig_caption: yes
---

<style type="text/css">

h1.title{ /* Normal  */
      font-size: 24px;
  }

body{ /* Normal  */
      font-size: 13px;
  }

</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

***

Team ID: Team 2; 
Members: Kenneth Lee, Xialin Sang,  Rong Duan, Chen Zhang

***

### 1. Introduction
<!--------Describe the data (e.g. the purpose of STAR, experiment process (especially, sampling process) )
Explain the purpose of this analysis.
Summarize the result from your analysis, briefly.
Exploratory Data analysis (e.g. Missing pattern, report any interesting findings which is related to the purpose of your analysis and which can help readers to understand your analysis)---->

#### 1.1 Background
The effects of class sizes on student achievement is an important topic for policymakers in the American K-12 education system. To study the effects of class size on student achievement in the primary grades, the State Department of Education in Tennessee launched a four-year longitudinal class-size randomized study from 1985 to 1989 called The Student/Teacher Achievement Ratio (STAR). Over 7000 students in 79 schools participated in this project. We highlight the features of the experiment process in the study below. 

* All participating schools had to agree to the random assignment of teachers and students to different class conditions: small class (13 to 17 students per teacher), regular class (22 to 25 students per teacher), and regular-with-aide class (22 to 25 students with a full-time teacher's aide) [5]. 

* The assignments of various class types were initiated as the students entered school in kindergarten and continued through third grade [5]. 

* Each school must provide enough kindergarten students to be assigned to three numerous class types in order to participate in the project STAR [5]. 

* The student achievement is measured annually via Stanford Achievement Tests (SATs) during the spring term on testing dates specified by the Tennessee state.[5]. 

* Students moving from a school involved in STAR to another participating school were assigned to the same type of class as they had participated in previously. Also, it is possible that the size of a regular class can be as small as the small class type as students move out of the participating schools[5].

* Besides class size and teacher aides, there were no other experimental changes involved in the study [5].

* There were three schools resigned from the project STAR at the end of kindergarten, so that there were only left with 76 schools in the 1st-grade level [5].

Our primary scientific question of interest is whether there is a treatment effect of assigning various class types to the average math scaled scores in a 1st-grade class level. We implement exploratory data analysis, two-way ANOVA model, model diagnostics, hypothesis testing. In the end, we will discuss any causal statements that could possibly be made based on our analysis and assumptions and the differences between a student-level and a class-level analysis on this STAR dataset.

This work shows that the treatment effect of the class type does exist in a class level for this dataset. We also show that it is possible to make causal statements based on our analysis.

#### 1.2 Exploratory Data Analysis
 <!----1. (e.g. Missing pattern, report any interesting findings which is related to the purpose of your analysis and which can help readers to understand your analysis) 
 
 Explore math scaled scores in the 1st with teachers as the unit--->

The original dataset has 11601 observations and 379 attributes. It describes the demographics of the students and teachers, class type assignment, the participating school and class identifiers, and test scores. We first examine if there are any missing values in the data. Then, we will explore the data with teachers as the unit for the 1st-grade students' math scaled scores. We summarize some the findings below:

* ***High level of missing values for teacher identifiers: ***Before we explore the data with teachers as the unit, we have found that there are 4772 observations that are missing teacher ID in the data. Above 40% of the total number of observations do not contain information related to teachers. We drop all the observations that do not have a teacher ID as we can hardly impute this identification information. Then, we are then left with 6829 observations. 

* ***Missing class type information in some schools: *** We have also found that there are four schools (ID: 244728, 244796, 244736, 244839) that do not have at least one observation per class type, which contradicts with the experimental design. We then drop the observations from these schools to ensure we have at least one observation per class type in each school.

* ***Small class types have higher average math scores: *** After we drop the observations that do not contain math scaled scores in a student level, we then take the average of the math scaled scores based on the remaining 6334 observations. We are then left with 325 observations with teachers as the unit. The distribution of the averaged math scores by class types is shown by `Appendix II. figure 2`. We can see that the averaged math scores are higher in general. 


### 2. Analysis Plan
<!----Describe your analysis plan (e.g. signficant level, posthoc analysis method, model diagnostic approach)
Define and explain your model
Specify the hypotheses
You can combine the analysis plan and result together in a single section. (i.e. You do not need to separate the data analysis section into two sections (data analysis, analysis result) )---->


#### 2.1 Two-way ANOVA Model

To see whether there is a treatment effect of the class type assignment in a class level, we will use a two-way ANOVA model. Our two-way ANOVA model is an additive model as specified below:

$Y_{ijk} = \mu_{..} + \alpha_{i} + \beta_{j} + \epsilon_{ijk}$ where $i = 1,...,3$, $j = 1,...,72$ and $k = 1,...,n_{ij}$

***Explanation of the notation***

* $Y_{ijk}$ denotes the average math scaled score of the ith class type and the jth school for the $k$th teacher.

* $\mu_{..}$ denotes the overall average of math scaled scores in the population across class types and schools that we try to estimate and this is an unknown parameter.

* $\alpha_{i}$ denotes the main effect of the $i$th class type.

* $\beta_{j}$ denotes the main effect of the $j$th school.

* $\epsilon_{ijk}$ denotes the random error in the $i$th class type, $j$th school for the $k$th teacher. This is an unobserved random variable.

* The index $i$ represents the levels of class type: small ($i=1$), regular ($i=2$), regular with aide ($i=3$). 

* The index $j$ represents the levels of the school indicator. Since we have only 72 schools in the data, we have  $j=1,...,72$.

* The index $n_{ij}$ denotes the total number of teachers in the $j$th school corresponding to the $i$th class type.

***The assumptions of the two-way ANOVA model*** 

* The random errors are assumed to be identically and independently distributed from a normal distribution with mean $0$ and variance $\sigma^{2}$.

* The outcomes are independent normal random variables with a common variance and with means equal to the overall average of math scaled scores across class types and schools. 

* The interaction effects are absent.

***Justification for the model choice****

We do not include the interaction term in our two-way ANOVA model because the interaction term is not of our primary interest. Another reason is that we are also concerned about the model complexity. Given that there is a limited number of observations within each combination of the class type and the school indicator via our exploratory data analysis, we may not have enough information to estimate the parameters. Also, we introduce a blocking factor $\beta_{j}$ into our model for controlling the source of varability; that is, we want to eliminate the effects due to variations among different schools while we are trying to determine the effects due to differences among class type assignments. Thus, this design of the experiment can give a greater accuracy of the model estimates [7].

#### 2.2 Model Diagnostics

Next, our model diagnostics will confirm whether the assumptions of our two-way ANOVA model hold such that this model is appropriate for the problem setting. 

* For the normality assumption, we will use a normal Q-Q plot and conduct a Shapiro–Wilk test to see if the residuals follow the normal distribution.

* For equal variance assumption, we will use a residual vs. fitted value plot to see if the residuals (the differences between the actual test scores and predicted test scores) have mean zero and equal variance. 

* The independence assumption will not be tested. Since the design of this ANOVA model is randomized block design, not only students are randomly assigned, teachers within each school are also randomly assigned to a certain class type. As a result, independence is satisfied.    

Furthermore, we would also like to confirm our assumption about absent or ignorable interaction effect between two factors by running the Tukey’s test for additivity.  


#### 2.3 Hypothesis Testing

As our primary interest is to determine whether there exists a treatment effect of the class types on the math scaled scores in class-level, we plan to use the F-test to test whether there is any main effect across class types. For our hypothesis testing, the significant level is set as 0.05. Our null hypothesis for the F-test is that $H_{0}: \alpha_{i} = 0$, no main effects across class types, with the alternative hypothesis $H_{a}:$ not all $a_{i}$ are zero, main effects exist based on some class types. Upon the rejection of the null hypothesis, we will then investigate the nature of the differences among the averaged test scores of the class types.

After knowing the overall F-test is significant for testing the main effect of class types, we may proceed to find out more specific information about where the difference should be accounted by comparing the averages of mean test scores across class types in 1st grade. Subsequently, we will use a multiple comparison analysis to determine where the differences among the averages of the mean scores on class types occur. Since the dataset does not have the same number of observations for each class type, it is suggested that we should use Tukey's procedure as it can give us a more precise estimation of the difference between the averages of the mean test scores from two different class types based on a narrower confidence interval [6].

### 3. Result

#### 3.1 Two-way ANOVA model

`Table 1` shows a summary of our ANOVA model. The sum of squares of schools shows the variability among the averaged test scores across schools, which is 128366. The more similar the average of the mean test scores of schools, the smaller this sum of squares tend to be. Across class types, we also see that the variation of the outcomes around their respective mean of the averaged test scores based on each class type is lower, which is about 6559. The smaller this value is, the smaller error variance we have. Similar to the usage of F-value, the p-value helps us understand whether there is a difference among the main effects of each class type or the difference happens due to chance. We will leave the detailed discussion of hypothesis testing in section `3.3`, which also utilizes the information from `Table 1`.


**Table 1: ANOVA Table**

|   Source  of Variation  |  Degrees of Freedom  |   Sum of Squares   |    Mean Square    |    F   |  p-value    |
|:------------|-----:|--------------:|--------------:|-------:|--------------------:|
|  Class Type      |  *2* |  6559       |  6559       | 22.6 |  **3.35e-06**      |
|  School  |  71  |  128366       |   1808       |    6.2    | **< 2e-16**                    |
|  Error/Residual   |  251  |  73131       |   290.2          |        |                     |

#### 3.2 Model Diagnostics

**Equal Variance Assumption: **According to `Appendix II. figure 3`, the residuals are scattered in between positive 50 (roughly) and negative 50 points. Visually, there seems to be more residuals lying in the positive extreme than negative extreme.  Other patterns are hard to detect from this graph. Therefore, we conducted Levene's Test for Homogeneity of Variance. From the test output, we can see that the p-value is less than 0.05. It indicates there is strong evidence suggesting that the variance across groups is different. This result makes sense because the residuals are unique, rather than the average of a group. In the appendix II., `figure 4` and `7` in appendix plot the residual against each factor. From these plots, we can examine the equal variance assumption in terms of the class types and school id. The variance of residual is not constant in terms of both factors and seems to be affected by the factor level.    

**Normality Assumption: **In `Appendix II. figure 3`, the QQ plot shows a heavy tail on both ends, and the normal distribution assumption is questionable. To further investigate this matter, we conducted the Shapiro-Wilk normality test. Since the p-value is less than the chosen significance level $0.05$, we should reject the null hypothesis and conclude that it is unlikely for the data to come from a normally distributed population.

In search for a remedy to the non-normality issue, we considered taking box-cox transformation of the response variable (math score).  However, the transformed response variable still rejects the null hypothesis at the $0.05$ significance level. Taking into account the fact that box-cox transformation undermines the interpretability of the data, we decided not to transform the response variable.


**Interaction Effects: **Lastly, we further conducted Tukey’s test for additivity for the ignorable interaction assumption since it is more appropriate for a randomized block design. The final test produced a more satisfactory result. With p-value equals to $0.42$, we can not reject the null hypothesis at the significance level 0.05. It is more likely that there is  no interaction between the two factors in the ANOVA model.    

#### 3.3 Hypothesis Testing

To conduct a F-test for the main effects of class types, the first step is to state the null hypothesis $H_{0}$: $\alpha_{1}=\alpha_{2}=\alpha_{3}$, where each $\alpha_{i}$ represent the average of the mean test scores within each class type. The alternative hypothesis $H_{\alpha}$: not all $\alpha_{i}$’s are equal. From `table 1`, we see that the pvalue is less than $0.05$. Therefore, we reject the null hypothesis at the significance level $0.05$ as the p-value suggests that the equality of the main effects of the class types happen less than 5% of the time. Therefore, we conclude that there is a main effect the class types. 

Then, we use Tukey's procedure to find out more information where the difference may come from. In the `Appendix II. Figure 8`, it gives us an intuition of confidence intervals for the difference in the means for all pairs of class types, and all pairs of school indicators. For example, the confidence interval (the interval at the bottom of the `Appendix II. figure 8`) that compares regular classes with regular classes with aide. As the 95% confidence interval includes zero, there is no statistically significant difference between the treatments. Then, we observe from the other two confidence intervals, again in `Appendix II. figure 8`, that the main effect can be attributed to the differences between small class types and other class types.  

### 4. Discussion
#### 4.1 Possibility of making any causal statements

Now we would like to make a causal statement based on the above analysis. If the following assumptions are examined to be qualified in the study, then we could make a causal inference based on the analysis. The assumptions are summarized below.

- **Stable unit treatment value assumption (SUTVA):** SUTVA has two implications: 1) No interference. That means the class type assignment of one teacher does not affect the potential outcomes of others. A class-level average score in math is not dependent on whether other teachers are assigned to a certain class type. For example, if a teacher in the regular class is discouraged because of the assignment of class types, they would not work as harder as usual, which could interfere with the experiment outcome. 2) The treatments are consistent - there is no different version of each treatment level [1]. In STAR, this assumption means all teachers in the same class type should use a similar way to teach, which makes sure the treatment is stable and the teaching quality is not accounted for the cause. This assumption excludes the unstable treatment cases for causal inference. However, it seems very difficult to implement the same teaching method in practice.

- **Randomization:**
With the double randomization of the students and teachers, we can fairly assume that the teachers and students are independent with the 3 class types. With a large sample size, the variation of performance students and teachers would not like to interfere with our outcome. Also, with the randomization of student assignment, we could ignore the potential influences of other factors (like gender, sex, etc.). 

- **Exchangeability:** 
Randomization experiment is expected to produce exchangeability [2]. Exchangeability means the potential outcome is independent with treatment. In our case, it means that the class type which a teacher assigned to is independent with the average math score in that class. In other words, because each teacher and student are distinctive individuals, the class type assignment does not dictate a different performance in the score. But potential outcome is not observed outcome. With exchangeability, we can conclude that the differences in scores among the 3 class types would exist in the whole population. Thus, we could use the association relationship to draw some causal inferences based on this assumption.


- **Positivity:**
Positivity is the assumption that any individual has a positive probability of receiving all levels of the treatment. In STAR, we just simply check whether every school has all 3 types of classes, and drop the schools which don’t have enough class types. This also corresponds to the randomized block design of our analysis.

- **Double-blind:**
To satisfy the double-blind assumption, the teachers and students should not have any preconception that there is a treatment effect among different class types. Otherwise, this information would disturb their educational performance. In the same way, if the scientists believed the class types have treatment effects, their analysis would be interfered by this preconception.

As long as the above assumptions hold and our analysis confirms that the main effects of different class types are significant, we can quantify the average causal effects due to class types. However, we recognize that there are many potential aspects of the experiment that can undermine the above assumptions. For example, missing values would invalidate the randomization. If the missing values are not randomly missing but because of some certain purpose, the exchangeability assumptions would not hold, which is difficult for us to draw a causal statement.

#### 4.2 Differences between student-level and class-level analysis

In the project 1, we used one-way ANOVA for our model to test whether there exists a treatment effect of the class type on the math scaled scores based a student-level analysis. The causal statement was made possible assuming the implementation of completely randomized design. In fact, the Stable Unit Treatment Value Assumption (SUTVA) is not plausible in the student-level study, because students are prone to peer influences. Violations of SUTVA complicate the regression and imputation approaches considerably, and we therefore primarily focus on teacher-unit in order to draw some causal inferences [3]. In this project, with teachers as the unit, the observed values are the averages of test scores within each class. It is ensured that the class type assignment of one teacher does not affect the potential outcomes of others.

In addition, we implemented a randomized block design in project 2 by adding the school identity as another factor. This is necessary because different schools would have pre-treatment effects on different levels, which interfere with the scores of each class type. Under the randomized block design setup, subjects within each block are randomly assigned to different treatments. Compared to the completely randomized design in project 1, this design reduces within-class type variability and potential confounding, producing better estimates of the treatment effects [4]. As a part of the two-way ANOVA model, we account for an additional source of variability coming from schools as the blocking factor. The previous within treatment variability is split into two sources: the reduced error variability and block variability. As a result, the treatment variability becomes larger compared to the reduced error variability. Thus, the class types can provide a better explanation to the differences in scores.

### Appendix I. Reference
[1] Marin Vlastelica Pogančić, 2019, "Causal vs. Statistical Inference", https://towardsdatascience.com/causal-vs-statistical-inference-3f2c3e617220, Max Planck Institute for Intelligent Systems

[2]Miguel A. Hernán, James M. Robins(2018).Causal Inference: What If.1420076167

[3]Guido W. Imbens & Donald B. Rubin, CAUSAL INFERENCE for Statistics, Social, and Biomedical Sciences An Introduction, chapter 9, ISBN 978-0-521-88588-1.

[4]Hanushek, Eric A. "Some findings from an independent investigation of the Tennessee STAR experiment and from other investigations of class size effects." Educational Evaluation and Policy Analysis 21.2 (1999): 143-163.

[5] C.M. Achilles; Helen Pate Bain; Fred Bellott; Jayne Boyd-Zaharias; Jeremy Finn; John Folger; John Johnston; Elizabeth Word, 2008, "Tennessee's Student Teacher Achievement Ratio (STAR) project", https://doi.org/10.7910/DVN/SIWH9F, Harvard Dataverse, V1, UNF:3:Ji2Q+9HCCZAbw3csOdMNdA== [fileUNF]

[6] Kutner, Michael H., et al. Applied linear statistical models. Vol. 5. New York: McGraw-Hill Irwin, 2005.

[7] (2008) Randomized Block Design. In: The Concise Encyclopedia of Statistics. Springer, New York, NY

### Appendix II. Figures


```{r EDA, echo=FALSE, warning=FALSE, message=FALSE}
# Load packages
library(multcompView)
library(ggplot2)
library(naniar)
library(dplyr)

star <- read.table("STAR_Students.tab",sep="\t", header=TRUE)
# Keep the columns only relevant to the first grade students
columns <- c("gender","g1classtype","g1schid","g1surban","g1tchid","g1tgen","g1trace","g1thighdegree","g1tcareer","g1tyears","g1tmathss")

data <- star[,columns]
# Change the colnames
colnames(data) <- c("Gender", "Class Type in Grade 1", "School ID","School Urbanicity","Teacher ID", "Teacher Gender", "Teacher Race", "Teacher Highest Degree", "Teacher Career Ladder","Teaching Experience", "Math Scale Score in 1st Grade")
#Missing Data
#vis_miss(data)
gg_miss_var(data, show_pct = TRUE) + labs(title = "Missing values proportion",y="Missing Proportion",x="Features")

```

***Figure 1***: *Missing Proportion Plot*: It shows the variables that have missing values in descending order. The x-axis shows the percentage of the missing values based on the overall number of observations.


```{r EDA2, echo=FALSE, warning=FALSE, message=FALSE}
# Boxplot
all_g1 <- c("g1classtype",'g1tmathss', "G1SCHID", "G1TCHID", "G1TGEN", "G1TRACE","g1thighdegree", "g1tcareer","g1tyears","g1classsize")
all_g1 <- tolower(all_g1)
boxdata<- star[, all_g1]

# Drop all the observations that have missing teacher id
boxdata <- boxdata[!is.na(boxdata$g1tchid),]

# Drop the school that does not have all 3 class types
drop_school <- c(244728, 244796, 244736, 244839)
boxdata <- boxdata[!(boxdata$g1schid %in% drop_school),]

# We will drop the observations that have math scores before we take the averages
boxdata <- boxdata[!is.na(boxdata$g1tmathss),]

data <- boxdata %>%group_by(g1tchid) %>%summarize_each(funs(mean))

data$g1classtype <- as.factor(data$g1classtype)
data$g1schid <- as.factor(data$g1schid)

p2 <- ggplot(data, aes(x=g1classtype, y=g1tmathss)) +
  geom_boxplot(fill='#A4A4A4', color="black")+
  theme_classic() + xlab("Class Type")+ ylab("Math Scaled Scores")+ggtitle("Math Scaled Scores by Class Type in class level")

#require(gridExtra)
#grid.arrange(plt1, plt2, nrow=2)
```

```{r hist, echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5, fig.align = "center"}
p3 <- qplot(data$g1tmathss, geom="histogram", main = "The distribution of the averaged math scores in class level", xlab= "Averaged Math Scores", ylab="Count") 
require(gridExtra)
grid.arrange(p2, p3, nrow = 1)
```

***Figure 2***: *Distribution plots.* Left panel: The distribution of average math scaled scores by class type. The class type is described on the x-axis (1: small class; 2: regular class; 3: regular class with aide). Both the highest averaged math score and the lowest math score belong to the small class type.
Right panel: The distribution of the averaged math scaled scores. The distribution of the averaged math scores seem to be roughly normal.


```{r anova, echo=FALSE, warning=FALSE, message=FALSE}
library(dplyr)
rawdata <- read.table("STAR_Students.tab",sep="\t", header=TRUE)
# dim(star)

# We keep only 4 columns
keep <- c("g1classtype", 'g1tchid', 'g1schid', 'g1tmathss')
data <- rawdata[,keep]


# Finding 1: drop all the teacher id that has missing values
# sum(is.na(data$g1tchid))# 4772 observations have missing teacher id

# Drop all the observations that have missing teacher id
data <- data[!is.na(data$g1tchid),]


# Finding 1: it isn't true that every school has 3 class types
# Check if every school has 3 class types
#for (i in unique(data$g1schid)){
#  if (sum(unique(data[data$g1schid==i,]$g1classtype)) != 6){
#    print(paste("School ID: ", i))
#    print(unique(data[data$g1schid==i,]$g1classtype))
#    print("-------------------")
#  }
#}

# Finding 2: it is true that every teacher only has 1 school id
# Check if every teacher has the same school id
#for (k in unique(data$g1tchid)){
#  print(unique(data[data$g1tchid==k,]$g1schid))
#  print("-------------------")
#}

# Drop the school that does not have all 3 class types
drop_school <- c(244728, 244796, 244736, 244839)
data <- data[!(data$g1schid %in% drop_school),]

# We will drop the observations that have math scores before we take the averages
dummy <- data[!is.na(data$g1tmathss),]
# Check to see if we still have all class types for each school
#for (i in unique(dummy$g1schid)){
#    print(unique(dummy[dummy$g1schid==i,]$g1classtype))
#    print("-------------------")
#}

# Get average scores by each teacher given each teacher has only 1 class type and 1 school id 
data<- dummy %>%
  group_by(g1tchid) %>%
    summarize_each(funs(mean))

data$g1schid <- as.factor(data$g1schid)
data$g1tchid <- as.factor(data$g1tchid)
data$g1tchid <- as.factor(data$g1classtype)

# ANOVA model fitting
lm.fit <- lm(g1tmathss ~ g1classtype + g1schid,data=data)
anova.fit <- aov(lm.fit)
```



```{r twowayANOVA, include=FALSE}
library(ggplot2)
library(gridExtra)
library(MASS)
library(car)
install.packages("nortest")
library(nortest)
```


```{r twowayANOVA_1, include=FALSE, result='hide'}
#Rename columns
names(data)[1] <- "teacher id"
names(data)[2] <- "class types"
names(data)[3] <- "school id"
names(data)[4] <- "math scores"

#Two-way ANOVA model
additive_model <- aov(`math scores`~`class types`+`school id`,data=data)
trans_additive_model <- aov((`math scores`)^(-2)~`class types`+`school id`,data=data)

full_model<- aov(`math scores`~`class types`*`school id`,data=data)
summary(additive_model)
summary(full_model)
```

```{r twowayANOVA_2, include=FALSE}
# tables
# model.tables(additive_model, "mean")
```

```{r interactions_1, include=FALSE}
# Test whether interaction effects are present
anova(additive_model, full_model)
# result is non-significant
# this means it has interaction effects...?
```

```{r interactions_2, include=FALSE}
# Tukey's test for interaction
tukeys.add.test <- function(y,A,B){
## Y is the response vector
## A and B are factors used to predict the mean of y
## Note the ORDER of arguments: Y first, then A and B
   dname <- paste(deparse(substitute(A)), "and", deparse(substitute(B)),
                  "on",deparse(substitute(y)) )
   A <- factor(A); B <- factor(B)
   ybar.. <- mean(y)
   ybari. <- tapply(y,A,mean)
   ybar.j <- tapply(y,B,mean)
   len.means <- c(length(levels(A)), length(levels(B)))
   SSAB <- sum( rep(ybari. - ybar.., len.means[2]) * 
                rep(ybar.j - ybar.., rep(len.means[1], len.means[2])) *
                tapply(y, interaction(A,B), mean))^2 / 
                  ( sum((ybari. - ybar..)^2) * sum((ybar.j - ybar..)^2))
   aovm <- anova(lm(y ~ A+B))
   SSrem <- aovm[3,2] - SSAB
   dfdenom <- aovm[3,1] - 1
    STATISTIC <- SSAB/SSrem*dfdenom
    names(STATISTIC) <- "F"
    PARAMETER <- c(1, dfdenom)
    names(PARAMETER) <- c("num df", "denom df")
    D <- sqrt(SSAB/  ( sum((ybari. - ybar..)^2) * sum((ybar.j - ybar..)^2)))
    names(D) <- "D estimate"
    RVAL <- list(statistic = STATISTIC, parameter = PARAMETER, 
        p.value = 1 - pf(STATISTIC, 1,dfdenom), estimate = D,
        method = "Tukey's one df F test for Additivity", 
        data.name = dname)
    attr(RVAL, "class") <- "htest"
    return(RVAL)
   }
```

```{r interactions_3, include=FALSE, message=FALSE}
tukeys.add.test(data$`math scores`,data$`class types`,data$`school id`)
# pvalue = 0.4235, nonsignificant. larger than 0.05. Can not reject the null hypothesis.
# null is D = 0 no interaction. # We don't want to reject the null hypothesis
```


```{r modelDiag1, echo=FALSE, warning=FALSE, message=FALSE}
# par(mfrow=c(2,2))
# plot(additive_model)
```

```{r diag_boxcox, echo=FALSE, warning=FALSE, message=FALSE, fig.show='hide'}
boxcox(`math scores`~`class types`+`school id`, data = data)
```

```{r chen, echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5, fig.align = "center"}
# ggplot(additive_model , aes(sample = rstandard(additive_model))) + geom_qq(color = 'dimgrey', alpha = 0.8) + stat_qq_line(color = 'dimgrey')

# Residual against fitted values plot
# checking equal variance of the error term
p1 <- ggplot(additive_model)+ geom_point(aes(x=.fitted,y=.resid), color="dimgrey", alpha = 0.8)+ labs(title="Residual Plot", x="fitted scores", y="residuals")+theme(
    plot.title = element_text(hjust = 0.5, size = 14),    # Center title position and size
    plot.subtitle = element_text(hjust = 0.5),            # Center subtitle
    plot.caption = element_text(hjust = 0, face = "italic")# move caption to the left
  ) 


# QQ plot
# checking normality of the error term
p2 <- ggplot(additive_model , aes(sample = rstandard(additive_model))) + geom_qq(color = 'dimgrey', alpha = 0.8) + stat_qq_line(color = 'dimgrey')+ labs(title="QQ Plot", x="theoretical quantile", y="standardized residual quantile")+theme(
    plot.title = element_text(hjust = 0.5, size = 14),    # Center title position and size
    plot.subtitle = element_text(hjust = 0.5),            # Center subtitle
    plot.caption = element_text(hjust = 0, face = "italic")# move caption to the left
  ) 

grid.arrange(p1, p2, nrow = 1)
```

***Figure 3: *** *Model diagnostic plots.* Left panel: residual versus fitted values. Right panel: QQ plot with residuals.


```{r diag_residual_qqplot, echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5, fig.align = "center"}
# ggplot(additive_model , aes(sample = rstandard(additive_model))) + geom_qq(color = 'dimgrey', alpha = 0.8) + stat_qq_line(color = 'dimgrey')

# Residual against fitted values plot
data_plot <- data.frame(data)
data_plot['residual'] = additive_model$residuals

p3 <- ggplot(data_plot)+ geom_point(aes(x=class.types,y=residual), color="dimgrey", alpha = 0.8)+ labs(title="Figure 4.1: Residual vs. class types", x="class types", y="residuals") +theme(        axis.text.x=element_blank(),axis.ticks.x=element_blank())

# Residual against fitted values plot
p4 <- ggplot(data_plot)+ geom_point(aes(x=school.id,y=residual), color="dimgrey", alpha = 0.8)+ labs(title="Residual vs. school id", x="school id", y="residuals") + theme(        axis.text.x=element_blank(),axis.ticks.x=element_blank())

grid.arrange(p3, p4, nrow = 1)
```

***Figure 4: *** *More residual plots.* Left panel: esidual vs. class types Right panel: Residual vs. school id.

```{r modelDiag2qqplot, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Residual against fitted values plot
# checking equal variance of the error term

# bartlett.test(`math scores`~`class types`, data = data)
# bartlett.test(`math scores`~`class types`*`school id`, data)

# Levine test
data$`class types` <- as.factor(data$`class types`)
data$`school id` <- as.factor(data$`school id`)
leveneTest(`math scores`~`class types`*`school id`, data = data)
# From the output above we can see that the p-value is not less than the significance level of 0.05. This means that there is no evidence to suggest that the variance across groups is statistically significantly different. Therefore, we can assume the homogeneity of variances in the different treatment groups.



# data$res.abs=abs(additive_model$residuals)
# summary(aov(res.abs~`class types` + `school id`,data=data))



shapiro.test(x = trans_additive_model$residuals)
shapiro.test(x = additive_model$residuals)

# null - > Normality -> non significant - However, we get significant result
# A p-value less than 0.05 (typically ≤ 0.05) is statistically significant
#  if the p value is greater than the chosen alpha level, then the null hypothesis that the data came from a normally distributed population can not be rejected (e.g., for an alpha level of .05, a data set with a p value of less than .05 rejects the null hypothesis that the data are from a normally distributed population)

ad.test(trans_additive_model$residuals)
ad.test(additive_model$residuals)

# null -> normally distributed
# if pvalue is low then we reject the hypothesis
```

```{r Tukey_kramer,echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5, fig.align = "center"}
# library
#install.packages('multcompView')
library(stats)
library(multcompView)
#Tukey test to study each pair of treatment
fit1=aov(data$`math scores`~as.factor(data$`class types`))
TUKEY=TukeyHSD(fit1,ordered="TRUE" )
#TUKEY
# Tuckey test representation :
 plot(TukeyHSD(fit1), col="gray31") 
```

***Figure 5:*** *The Confidence Interval of Multiple Comparisons of Means.* The confidence interval of Multiple Comparisons of the averages of the class types (1:small class, 2: regular class, 3: regular class + aide). 













